{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Lab XX: An Introduction to Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd, numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## There are two main groups of activities you can do with Clustering\n",
    "\n",
    "If you are interested in using K-Means to describe your (unlabeled) data, you can use Kmeans to split your data into $k$ clusters and then analyze the clusters. This is what we are going to do in this notebook.\n",
    "\n",
    "You can also use a similar process called K-nearest neighbours (KNN) for classification tasks. In KNN, clusters are formed with **labeled** data and then **unlabeled** data is categorized into the clusters.\n",
    "\n",
    "The good news is that although K-Means is a machine learning algorithm, it is reletively simple to implement. You don't have to be familiar with the math behind K-Means, but a conceptual demonstration of the algorithm from UC Berkeley's Data 100 lecture on clustering is available [here](lec23_clustering_d100_excerpt.pdf). \n",
    "\n",
    "A guide on the algorithm is also documented [here](https://scikit-learn.org/stable/modules/clustering.html#k-means) by `scikit-learn`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### The Data\n",
    "For the purposes of this exercise, we are going to be using a dataset of transit stops in Toronto, along with census variables aggregated from the census tract level to the neighbourhood in which the stops are located.\n",
    "\n",
    "It was created in the `create_stops_data.ipynb` notebook, which you are welcome to review but you do not need to recreate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stops = pd.read_csv(\"toronto_stops.csv\").drop(columns = [\"Unnamed: 0\"])\n",
    "stops.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The K-Means algorithm only works with numeric data (floats and ints, in Python). Omit the columns with the stop id and neighbourhood name so that the data being put into the algorithm only consists of the numeric stop charactaristics.\n",
    "\n",
    "You might notice that a lot of the rows have the same values for the demographic variables. That is because the demographic variables averaged at the neighbourhood level- and there are many transit stops in the same neighbourhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = stops.iloc[:, 2:]\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "This code below creates a `KMeans` object with $k$ clusters (`n_clusters`). This notebook was written using `n_clusters = 4`, but you are welcome to change this variable to 3, 5, 6, 7-- whatever seems like a reasonable number of distinct clusters in Toronto based on the variables selected. The data `X` is then fitted to the Kmeans object, which generates a set of 9,027 labels- one label for each stop!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# try running the notebook with various values for n_clusters\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(X)\n",
    "y_kmeans = kmeans.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Make a column called `cluster` in the `stops` dataset to see the labeled data. There are $k$ unique entries in the `y_kmeans` array, one for each cluster that you specified above. It makes a bit more sense to think about the clusters as beginning with cluster 1 instead of cluster 0, so let's shift the array by 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.unique(y_kmeans+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stops[\"cluster\"] = y_kmeans+1\n",
    "stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Determining the best number of clusters\n",
    "\n",
    "If we use too many clusters, we will have too few datapoints in each cluster and the clusters will not be meaningful. If we have not enough, the data in each of the clusters will be too disparate. There are a number of ways to do this, but for now we will use two of the most common, reliable, and easy to implement methods to determine the optimal number of clusters to describe the data. \n",
    "\n",
    "### Elbow plot\n",
    "\n",
    "An elbow plot shows the number of clusters vs. the distortion of the dataset. The **distortion** is calculated as the weighted sum of squared distances from each data point to its center. Typically, the squared Euclidean distance is used. A higher distortion indicates greater distances *between* the clusters, meaning that they are more distinct (that is good!). \n",
    "\n",
    "As a formula, this looks like:\n",
    "\n",
    "$$\\text{Distortion} = \\sum_{i=1}^n\\frac{1}{m(i)}\\sum_{j=1}^{m(i)}(x_j - z_i)^2$$\n",
    "\n",
    "where $i$ iterates over the number of clusters, $m(i)$ is the number of points in cluster $i$, $j$ iterates over all points assigned to the $i$th cluster, and $z_i$ is the center of cluster $i$. \n",
    "\n",
    "This plot shows the average distortion for up to 10 different clusters. What do you notice about the shape of the graph?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# k means determine k\n",
    "distortions = []\n",
    "K = range(1,10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(X)\n",
    "#     kmeanModel.fit(Xx)\n",
    "    distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])\n",
    "# Plot the elbow\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k (number of clusters)')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Rather than being smooth, the plot is made from distinct segments, and has \"elbow shaped\" corners. These are the values where there is a significant reduction in distortion as a result of an increase in number of clusters. This is an indication that we should stop dividing the data into further clusters. From this elbow plot, it seems that the optimal number of clusters for this dataset is around 4. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Silhouette score\n",
    "We can also use a metric called a silhouette score, and visualize clustered data projected onto **two** features at a time. The silhouette score measures the degree to which a given datapoint is correctly classified to a given cluster. The silhouette score ranges from [-1,1]. Scores close to -1 indicate that many points are in the wrong cluster, suggesting that there are too few or too many clusters in the model. Scores close to +1 indicate that most points are closely matched to their own clusters, and poorly matched to other clusters. This is what we want, so higher silhouette scores are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y=X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# experiment by changing the end of the range - no more than 10 should be sufficient.\n",
    "# Remember that (start, stop) are (inclusive, exclusive)\n",
    "range_n_clusters = np.arange(2,6)\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(y) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 377 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=377)\n",
    "    cluster_labels = clusterer.fit_predict(y)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(y, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(y, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\", fontsize=18)\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\", fontsize=16)\n",
    "    ax1.set_ylabel(\"Cluster label\", fontsize=16)\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(y.iloc[:, 0], y.iloc[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\", fontsize=18)\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\", fontsize=16)\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\", fontsize=16)\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=18, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "All of these clusters look pretty good! What happens if you keep increasing the number of clusters? A silhouette score above 0.5 is considered a good indication that the data is separable (classifiable into clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Unsuccessful Clustering\n",
    "\n",
    "A silhouette score that is a lot less than 0.5 is an indication that the number of clusters is wrong, or that the data is not naturally clusterable. If you get a silhouette score of less than 0.5 with a dataset in the future, consider transforming your data or using a different approach. Here is an example of what your results might look like if your data is not easily clusterable. \n",
    "\n",
    "Let's say you have a subset of the `stops` dataset that excludes all population density, Gini index, average monthly shelter costs, and average dwelling value so that you're left only with the `pct_` columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stops.columns[~stops.columns.str.contains(\"pct_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = stops[stops.columns[stops.columns.str.contains(\"pct_\")].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "range_n_clusters = [2, 3, 4, 5]\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    # Create a subplot with 1 row and 2 columns\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(18, 7)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    ax1.set_ylim([0, len(y) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 377 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=377)\n",
    "    cluster_labels = clusterer.fit_predict(y)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(y, cluster_labels)\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(y, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\", fontsize=18)\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\", fontsize=16)\n",
    "    ax1.set_ylabel(\"Cluster label\", fontsize=16)\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax2.scatter(y.iloc[:, 0], y.iloc[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    ax2.set_title(\"The visualization of the clustered data.\", fontsize=18)\n",
    "    ax2.set_xlabel(\"Feature space for the 1st feature\", fontsize=16)\n",
    "    ax2.set_ylabel(\"Feature space for the 2nd feature\", fontsize=16)\n",
    "\n",
    "    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                  \"with n_clusters = %d\" % n_clusters),\n",
    "                 fontsize=18, fontweight='bold')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Analyzing the Clusters\n",
    "Let's look at some summary statistics of the 3 clusters and see how they are similar to and different from each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster1 = stops[stops[\"cluster\"] == 1]\n",
    "cluster2 = stops[stops[\"cluster\"] == 2]\n",
    "cluster3 = stops[stops[\"cluster\"] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cluster1.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cluster2.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cluster3.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. How do each of the clusters differ from each other and how are they the same? Do these differences help you understand the data better for a study or research project? Why or why not?\n",
    "\n",
    "2. How much and what kind of information was lost when we clustered the transit stops?\n",
    "\n",
    "3. What are some other applications of clustering related to the urban domain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
