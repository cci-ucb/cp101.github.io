{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Lab 7 - Web Scraping\n",
    "---\n",
    "In today's lab, we are going to download data from the internet using an API. API stands for application programming interface. Companies often create APIs as a way to allow users to more directly interact with their servers to retrieve data. Today, we are going to be using CKAN's API to download data from the City of Toronto's Open Data Portal to get some experience working with larger datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import warnings\n",
    "import requests\n",
    "import sys\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO\n",
    "\n",
    "# Ensure that Pandas shows at least 280 characters in columns, so we can see full tweets\n",
    "pd.set_option('max_colwidth', 280)\n",
    "\n",
    "%matplotlib inline\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Setup\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# toronto public library info\n",
    "\n",
    "# Toronto Open Data is stored in a CKAN instance. It's APIs are documented here:\n",
    "# https://docs.ckan.org/en/latest/api/\n",
    "\n",
    "# To hit our API, you'll be making requests to:\n",
    "base_url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca\"\n",
    "\n",
    "# Datasets are called \"packages\". Each package can contain many \"resources\"\n",
    "# To retrieve the metadata for this package and its resources, use the package name in this page's URL:\n",
    "url = base_url + \"/api/3/action/package_show\"\n",
    "params = { \"id\": \"library-branch-general-information\"}\n",
    "package = requests.get(url, params = params).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print the metadata\n",
    "[x for x in package[\"result\"][\"resources\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "That's a lot of information compared to what we want- a simple dataset of library information! There are some important fields here to take note of that will guide how you download the information through the API. Note that the first resource has `datastore_active == True`. This means an instance of the data is stored on the Open Data portal's database. Not all records will have this value as `True`, as you can see below. For now, we will download the instance where this is true, but later in the lab we will learn what to do when the data is stored elsewhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To get resource data:\n",
    "for idx, resource in enumerate(package[\"result\"][\"resources\"]):\n",
    "\n",
    "       # for datastore_active resources:\n",
    "    if resource[\"datastore_active\"]:\n",
    "\n",
    "        # To get all records in CSV format:\n",
    "        url = base_url + \"/datastore/dump/\" + resource[\"id\"]\n",
    "        resource_dump_data = requests.get(url).text\n",
    "        tpl_libraries = pd.read_csv(StringIO(resource_dump_data), sep=\",\")\n",
    "tpl_libraries.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Toronto Open Data is stored in a CKAN instance. It's APIs are documented here:\n",
    "# https://docs.ckan.org/en/latest/api/\n",
    "\n",
    "# To hit our API, you'll be making requests to:\n",
    "base_url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca\"\n",
    "\n",
    "# Datasets are called \"packages\". Each package can contain many \"resources\"\n",
    "# To retrieve the metadata for this package and its resources, use the package name in this page's URL:\n",
    "url = base_url + \"/api/3/action/package_show\"\n",
    "params = { \"id\": \"rain-gauge-locations-and-precipitation\"}\n",
    "package = requests.get(url, params = params).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's look at which resources are available from this package\n",
    "[x for x in package[\"result\"][\"resources\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To get resource data, iterate over the package\n",
    "for idx, resource in enumerate(package[\"result\"][\"resources\"]):\n",
    "\n",
    "    # a CKAN value to indicate it can be downloaded\n",
    "    if not resource[\"datastore_active\"]:\n",
    "        # the name/url of the resource we want\n",
    "        if resource['name'] == \"precipitation-data-2022\":\n",
    "            # get its url and read it into a pandas dataframe in memory\n",
    "            resource_url = resource[\"url\"]\n",
    "            resource_data = pd.read_csv(resource_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "If you are having trouble with the previous cell, you can read in an already downloaded version of the dataset using the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resource_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running into errors downloading these tweets, uncomment and run the following cell to load in tweets that we scraped earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This accesses the first (zero index) element of the list\n",
    "gentrification[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an example of another `python` data structure called a *dictionary*. Dictionaries store *values* by associating them with a *key* rather than by an integer index. You can access the values stored in a dictionary using bracket notation just like a list. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this dictionary, the keys are strings, and the values are all numbers\n",
    "d = {'a': 1,\n",
    "    'b': 2,\n",
    "    'c': 3}\n",
    "\n",
    "d['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "### <font color='red'> Minute 4:20\n",
    "---\n",
    "The dictionary we were looking at above is a little bit hard to interpret because there dictionaries nested inside of some our keys. We can look only at the first level of keys in our dictionary by using the `.keys()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gentrification[0].keys()\n",
    "# You can also use .values() to access all of the values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, Twitter by default does not attach geographic data to the metadata of each tweet. To get around this, we can use the location associated to the account of each poster. First, we want to extract only the parts of the data that are relevant to what we are looking for. To do this, we first need to turn our list of dictionaries into a `pandas DataFrame`. Fortunately, there is a function that can do this easily for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gentrification_df = pd.DataFrame(gentrification)\n",
    "tpl_libraries.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Next, we want to extract out only the columns that are relevant to us. Discarding columns that do not help us answer our question can be helpful because it prevents the computer from having to do unnecessary computations. However, if we want to be able to connect any conclusions we make after we get rid of columns, it is helpful to keep an identifying column in your `DataFrame` even if you are not performing analyses on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = tpl_libraries[['id', 'user']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a closer look at an element of the `user` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users.loc[0, 'user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "In each row, the `user` column contains another dictionary with information about the user who posted the tweet. We can access the user's location using the `location` key.\n",
    "\n",
    "The strategy that we are going to use to extract the locations from each user will be to iterate through the rows of `users`; at each row we will add the tweet id and the user location to a new dictionary. This dictionary will then be added to a list. Once we have iterated through all of the rows of `users`, we will convert our final list of dictionaries into a `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list.\n",
    "locations_list = list()\n",
    "\n",
    "for i in range(len(users)):\n",
    "    # Create an empty dictionary.\n",
    "    new_entry = {}\n",
    "    # Copy the tweet id into the new dictionary.\n",
    "    new_entry['id'] = users.loc[i, 'id']\n",
    "    # Create a new key ('location') and assign it to the location of the user who\n",
    "    # wrote that tweet.\n",
    "    new_entry['location'] = users.loc[i, 'user']['location']\n",
    "    # Append the dictionary as another element of our list.\n",
    "    locations_list.append(new_entry)\n",
    "    \n",
    "# Transform our list into a DataFrame. As before, each element of the list becomes\n",
    "# a row in the DataFrame, and each key becomes a column.\n",
    "all_locations = pd.DataFrame(locations_list)\n",
    "\n",
    "# Display the first 10 tweets.\n",
    "all_locations.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Clearly this isn't a foolproof method, since the location associated with an account may have little bearing on the actual location from which a tweet was posted. Also, not all users have a specific location connected to their account. Depending on the data you have pulled from Twitter, you may also notice that some of the \"locations\" are not actually real places. We can do a bit of data cleaning to filter out the rows that contain true locations. First, let's get rid of the rows that do not contain any text at all in the `location` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame with the columns 'location' and 'id'\n",
    "no_empties = pd.DataFrame(columns = ['id', 'location'])\n",
    "for i in range(len(all_locations)):\n",
    "    # This filters out tweets whose location column is an empty string.\n",
    "    if all_locations.loc[i, \"location\"] != '':\n",
    "        no_empties = no_empties.append(all_locations.loc[i,:])\n",
    "no_empties.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks pretty good! We would still like to filter through our locations for places that actually exist. Let's use the `.groupby()` method to take a look at what locations we have in our data.\n",
    "\n",
    "The `.groupby()` method takes in a table, a column, and optionally, an aggregate function (the default is count() which counts how many rows have the same value for the column we are grouping by. Other options include sum() and max() or min()). Groupby goes through each row, looks at the column that has been given to it of the current row, and groups each row based on if they have the same value at given column. After it has a list of rows for each distinct column value, it applies the aggregate function for each list, and returns a table of each distinct column value with the aggregate function applied to the rows that corresponded with the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_empties.groupby('location').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you scroll through this list, you will likely see a whole litany of \"locations\" that do not resemble locations since the user is allowed to write whatever they like as their location. We do not have time to day to sort through all of these right now, so we are goign to move on to a few other techniques that we can use to analyze these kinds of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Since many of the tweets we scraped earlier do not have useful locations, we may want to filter by location when we ask the API for tweets. We can use the same function as before, using the optional `location` argument. The format of the location argument is `\"latitude,longitude,radius\"`. The following code searches for tweets hashtagged \"gentrification\" within a 5 km radius of the Temescal Oakland area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gentrification_oak = download_recent_tweets_by_hashtag(hashtag = \"gentrification\",\n",
    "                                                       keys = keys,\n",
    "                                                       location = \"37.829314,-122.264433,5km\",\n",
    "                                                       count = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running into errors downloading these tweets, uncomment and run the following cell to load in tweets that we scraped earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your turn:** Let's use the procedure we went through earlier to find the most common user location in the `gentrification_oak` tweets. We've provided some starter code, but you need to fill in wherever you see a `...`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_oak = pd.DataFrame(gentrification_oak)\n",
    "users_oak = ... # select columns of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_list_oak = list()\n",
    "for i in range(len(users_oak)):\n",
    "    new_entry = {}\n",
    "    new_entry['id'] = users_oak.loc[i, 'id']\n",
    "    new_entry['location'] = users_oak.loc[i, 'user']['location']\n",
    "    locations_list_oak.append(...) # we want to add the new entry to our list\n",
    "all_locations_oak = ... # turn the list into a DataFrame\n",
    "\n",
    "no_empties_oak = pd.DataFrame(columns = ['id', 'location'])\n",
    "for i in range(len(all_locations_oak)):\n",
    "    if all_locations_oak.loc[i, \"location\"] != '':\n",
    "        no_empties_oak = no_empties_oak.append(all_locations_oak.loc[i,:])\n",
    "        \n",
    "grouped_locations = ...\n",
    "\n",
    "# This finds the number of repeats of the most common location.\n",
    "max_number_of_tweets = grouped_locations['id'].max()\n",
    "\n",
    "most_common_location = grouped_locations[grouped_locations['id'] == max_number_of_tweets]\n",
    "\n",
    "# most_common_location is a DataFrame with one item. This access all of the indices in the\n",
    "# DataFrame, then takes the first (and only) one.\n",
    "most_common_location.index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Temporal Data\n",
    "---\n",
    "Another facet of urban data that you may want to analyze is the time at which they were posted. Currently, the only way we have information about the time the tweets were posted is in the `'created_at'` column, which is a string. As you may remember from the Introductory lab, `python` compares strings by assigning values to the letters themselves based on their position in the alphabet. We want to convert these strings to `datetime` objects, which will tell `python` at what time tweets were posted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_time = pd.DataFrame(gentrification_oak)[['id', 'created_at']]\n",
    "post_time['time'] = pd.to_datetime(post_time['created_at'])\n",
    "post_time['time'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that each string has been converted into a `datetime` object, we can extract the day, hour, minute, etc. of each time point like so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_time.loc[0, 'time'].day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_time.loc[0, 'time'].hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_time.loc[0, 'time'].minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we are not adding parentheses at the end of each line. That is because the `.day`, `.hour`, and `.minute` are not *functions* we are calling, but rather *attributes* of the particular `datetime` object. If we want to look at the time of day that people tend to tweet about #gentrification, we can extract these attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_time['hour'] = [post_time.loc[i, 'time'].hour + post_time.loc[i, 'time'].minute/60 +\n",
    "                     post_time.loc[i, 'time'].second/3600 for i in range(len(post_time))]\n",
    "post_time['hour'].hist()\n",
    "plt.xlabel(\"Hour (UTC)\")\n",
    "plt.ylabel(\"Number of Tweets\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What observations or trends do you notice about this graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What could be improved about this graph or the process we used to obtain the data that generated it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Sentiment Analysis\n",
    "---\n",
    "We can use the words the tweets to measure the sentiment, or the positive/negative feeling generated by the tweet. To do so we will be using the [VADER (Valence Aware Dictionary and sEntiment Reasoner)](https://github.com/cjhutto/vaderSentiment), which is a rule-based sentiment analysis tool specifically designed for social media. It even includes emojis! Run the following cell to load in the lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vader = load_vader()\n",
    "vader.iloc[500:510, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The more positive the polarity of a word, the more positive feeling the word evokes in the reader. All of the words in `vader` are all lowercase, while many of our tweets are not. We need to modify the text in the tweets so that the words in our tweets will match up with the words stored in `vader`. Additionally, we need to remove punctuation since that will cause the words to not match up as well. We will put these modified tweets into another column in our `DataFrame` so that we can still have access to them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our columns of interest\n",
    "tweets_and_retweets = pd.DataFrame(gentrification_oak)[['id', 'text', 'retweet_count']]\n",
    "\n",
    "# Set the index of the DataFrame to the tweet ID. This step is necessary\n",
    "# in order to use our utility functions.\n",
    "tweets_and_retweets.set_index('id', inplace = True)\n",
    "\n",
    "# Remove punctuation and lowercase tweets\n",
    "tweets_and_retweets['cleaned'] = clean_tweets(tweets_and_retweets['text'])\n",
    "\n",
    "tweets_and_retweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to merge our sentiment lexicon with our cleaned tweets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_and_retweets['polarity'] = compose_polarity(tweets_and_retweets, vader)\n",
    "tweets_and_retweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Next, we want to see if more polarizing tweets are retweeted more often. To do this, we can plot the `polarity` and `retweet_count` columns against each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tweets_and_retweets.plot('polarity', 'retweet_count', kind='scatter');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What conclusions can you draw about polarity and retweets from this graph? How does this compare with your assumptions?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Your turn!\n",
    "---\n",
    "If time allows, try these exercises on your own or as a class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Using the `gentrification_oak` tweets, make a histogram of the time of day the tweets were posted. Note that if you would like the x-axis of the plot to reflect the correct time of day, you will have to convert the time from UTC to PDT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2:** Try scraping tweets from multiple locations and the same hashtag. Make a histogram for each location and see if there are any differences in the distribution of polarity of the tweets. Feel free to use multiple cells to avoid querying the API repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
