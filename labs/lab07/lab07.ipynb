{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# Lab 7 - Introduction to Web APIs\n",
    "---\n",
    "\n",
    "Material adapted from CYPLAN 101, [D-Lab Web API Workshop](https://github.com/dlab-berkeley/Python-Web-APIs)\n",
    "\n",
    "In today's lab, we are going to download data from the internet using an API. API stands for application programming interface. Companies often create APIs as a way to allow users to more directly interact with their servers to retrieve data. Today, we are going to be using CKAN's API to download data from the City of Toronto's Open Data Portal to get some experience working with larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## What is a web API?\n",
    "\n",
    "APIs are often official services offered by companies and other entities, which allow you to directly query their servers in order to retrieve their data. Platforms like The New York Times, Twitter and Reddit offer APIs to retrieve data. In the case of this lab, you'll be working with Toronto Open Data stored on a CKAN instance, with its API documented [here](https://docs.ckan.org/en/latest/api/).\n",
    "\n",
    "## What if one isn't available? \n",
    "\n",
    "Then you would do web scraping, which generally requires parsing through HTML tags (BeautifulSoup is a popular library to help with that), simulating browser clicks using Selenium (creates an instance of a browser you can 'drive' using Python), and/or simply inspecting front end content. If you're curious to try it out, there is a workshop available from Berkeley's D-Lab [here](https://github.com/dlab-berkeley/Python-Web-Scraping) that you can use as a guideline for your own web scraping adventures.\n",
    "\n",
    "## Why wouldn't an API be available?\n",
    "\n",
    "Time, cost, resources, lack of user or developer interest. Do note that the absence of an API does not necessarily mean there is free reign to scrape a webpage. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Imports\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "import warnings\n",
    "import requests\n",
    "from ckan_utils import *\n",
    "import sys\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO\n",
    "\n",
    "%matplotlib inline\n",
    "import re\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Downloading data from the Internet\n",
    "\n",
    "**Note:**\n",
    "\n",
    "If you are having trouble with any of the following cells- fear not. You can read in an already downloaded version of the dataset(s) later on in the lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# toronto public library info\n",
    "\n",
    "# Toronto Open Data is stored in a CKAN instance. Its APIs are documented here:\n",
    "# https://docs.ckan.org/en/latest/api/\n",
    "\n",
    "# To use the API, you'll be making requests to:\n",
    "base_url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca\"\n",
    "\n",
    "# Datasets are called \"packages\". Each package can contain many \"resources\"\n",
    "# To retrieve the metadata for this package and its resources, use the package name in this page's URL:\n",
    "url = base_url + \"/api/3/action/package_show\"\n",
    "params = { \"id\": \"library-branch-general-information\"}\n",
    "package = requests.get(url, params = params).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "This is an example of another `python` data structure called a *dictionary*. Dictionaries store *values* by associating them with a *key* rather than by an integer index. You can access the values stored in a dictionary using bracket notation just like a list. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# In this dictionary, the keys are strings, and the values are all numbers\n",
    "d = {'a': 1,\n",
    "    'b': 2,\n",
    "    'c': 3}\n",
    "\n",
    "d['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "In the case of `package`, it is an example of a nested dictionary. To access its values, we need to use a key of a key. From the documentation, we see that the `result` of each package contains a list under the key `resources`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "package.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print the metadata\n",
    "[x for x in package[\"result\"][\"resources\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "There are some important fields here to take note of that will guide how you download the information through the API. Note that the first resource has `datastore_active == True`. This means an instance of the data is stored on the Open Data portal's database. Not all records will have this value as `True`, as you can see in the event that a resource can be downloaded in `csv`, `json`, or `xml` format. For now, we will download the instance where this is true, but later in the lab we will learn what to do when the data is stored elsewhere. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To get resource data:\n",
    "# iterate over the resources\n",
    "for idx, resource in enumerate(package[\"result\"][\"resources\"]):\n",
    "\n",
    "    # set a condition for when you want to access the resource:\n",
    "    if resource[\"datastore_active\"]:\n",
    "\n",
    "        # to get all records in CSV format, append the resource id to the base_url\n",
    "        url = base_url + \"/datastore/dump/\" + resource[\"id\"]\n",
    "        # do a GET request on the url and access its text attribute\n",
    "        resource_dump_data = requests.get(url).text\n",
    "        # read the raw csv text into a pandas dataframe to work with it\n",
    "        tpl_libraries = pd.read_csv(StringIO(resource_dump_data), sep=\",\")\n",
    "tpl_libraries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Now that we have information on the libraries, let's see if we can find out a little more about what goes on inside of them using the dataset `library-branch-programs-and-events-feed`. To do this, we will repeat the same actions as we did to retrive the library location data, but instead of writing everything over again, we can create a helper function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# repeat the setup from above\n",
    "#url = base_url + \"/api/3/action/package_show\"\n",
    "params = { \"id\": \"library-branch-programs-and-events-feed\"}\n",
    "package = requests.get(url, params = params).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[x for x in package[\"result\"][\"resources\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To get resource data:\n",
    "# iterate over the resources\n",
    "for idx, resource in enumerate(package[\"result\"][\"resources\"]):\n",
    "\n",
    "    if resource[\"datastore_active\"]:\n",
    "\n",
    "        # to get all records in CSV format\n",
    "        url = base_url + \"/datastore/dump/\" + resource[\"id\"]\n",
    "        # do a GET request on the url and access its text attribute\n",
    "        resource_dump_data = requests.get(url).text\n",
    "        # read the raw csv text into a pandas dataframe to work with it\n",
    "        tpl_events = pd.read_csv(StringIO(resource_dump_data), sep=\",\")\n",
    "tpl_events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Data Cleaning\n",
    "---\n",
    "Now, we want to extract out only the columns that are relevant to us. Discarding columns that do not help us answer our question can be helpful because it prevents the computer from having to do unnecessary computations. However, if we want to be able to connect any conclusions we make after we get rid of columns, it is helpful to keep an identifying column in your `DataFrame` even if you are not performing analyses on it.\n",
    "\n",
    "You can read about all of the columns under the data features tab [here](https://open.toronto.ca/dataset/library-branch-programs-and-events-feed/). It's good practice to read as much as you can about the metadata of a dataset, when and where it is available to minimize the amount of guesswork or reconstruction you'll have to do. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tpl_events.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tpl_events = tpl_events[['_id', 'title', 'startdate', 'enddate', 'starttime', 'endtime',\n",
    "       'length', 'library',  'description',  'id',\n",
    "       'rcid', 'eventtype1', 'eventtype2', 'eventtype3', 'agegroup1',\n",
    "       'agegroup2', 'agegroup3',  'lastupdated']]\n",
    "tpl_events.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Reshaping and pivoting dataframes\n",
    "\n",
    "But that's not all we can do to the data to make it easier to work with. It would be nice if the event type and age group columns were pivoted to one rather than three separate columns each. We can reshape dataframes into a 'long' format using the `melt` function. \n",
    "\n",
    "There is an important distinction to make in pandas datatypes. Normally, `None` is not a string in Python, it has a particular value which you can think of as null. But columns in Pandas must all be of a single type, and when a `None` is in a column with other strings, it too becomes a string. Therefore, to drop rows with `None`, you must use `!= \"None\"` rather than `!= None`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "event_types = tpl_events.melt(id_vars = [\"id\", \"library\"], value_vars = [\"eventtype1\", \"eventtype2\", \"eventtype3\"], value_name = \"eventtype\")\n",
    "event_types = event_types[event_types['eventtype'] != \"None\"].drop(columns = \"variable\")\n",
    "event_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "age_groups = tpl_events.melt(id_vars = [\"id\", \"library\"], value_vars = [\"agegroup1\", \"agegroup2\", \"agegroup3\"], value_name = \"agegroup\")\n",
    "age_groups = age_groups[age_groups['agegroup'] != \"None\"].drop(columns = \"variable\")\n",
    "age_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# join these back to tpl_events\n",
    "tpl_events_long = tpl_events.drop(columns = [\"eventtype1\", \"eventtype2\", \"eventtype3\", \"agegroup1\", \"agegroup2\", \"agegroup3\"]).merge(event_types, on = [\"id\", \"library\"], how = \"left\").merge(age_groups, on = [\"id\", \"library\"], how = \"left\")\n",
    "tpl_events_long.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Let's use the `.groupby()` method to summarize event types.\n",
    "\n",
    "The `.groupby()` method takes in a table, a column, and optionally, an aggregate function (the default is count() which counts how many rows have the same value for the column we are grouping by. Other options include sum() and max() or min()). Groupby goes through each row, looks at the column that has been given to it of the current row, and groups each row based on if they have the same value at given column. After it has a list of rows for each distinct column value, it applies the aggregate function for each list, and returns a table of each distinct column value with the aggregate function applied to the rows that corresponded with the column.\n",
    "\n",
    "Let's see if we can find the most popular library event type.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tpl_events_long.groupby('eventtype').size().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**Your turn:** Let's find the most common public library event type by age group for each branch. We've provided some starter code, but you need to fill in wherever you see a `...`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpl_events_long.groupby(['library', 'agegroup'])['eventtype'].agg(pd.Series.mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Temporal Data\n",
    "---\n",
    "Another facet of urban data that you may want to analyze is the time at which something occurs. `python` compares strings by assigning values to the letters themselves based on their position in the alphabet. We want to convert these strings to `datetime` objects, which will tell `python` at what time the precipitation was measured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "Notice that we are not adding parentheses at the end of each line. That is because the `.day` and `.month` are not *functions* we are calling, but rather *attributes* of the particular `datetime` object. If we want to look at the day of the month library events start on, we can extract these attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_date = tpl_events[['id', 'library', 'startdate']].drop_duplicates()\n",
    "start_date['time'] = pd.to_datetime(start_date['startdate'])\n",
    "start_date['day'] = start_date['time'].dt.day\n",
    "start_date['month'] = start_date['time'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date['day'].hist();\n",
    "plt.xlabel(\"Day of Month\");\n",
    "plt.ylabel(\"Number of Events\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**Question:** What observations or trends do you notice about this graph?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**Question:** What could be improved about this graph or the process we used to obtain the data that generated it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Sentiment Analysis\n",
    "---\n",
    "We can use the words the tweets to measure the sentiment, or the positive/negative feeling generated by the description text. To do so we will be using the [VADER (Valence Aware Dictionary and sEntiment Reasoner)](https://github.com/cjhutto/vaderSentiment), which is a rule-based sentiment analysis tool specifically designed for social media. It even includes emojis! Run the following cell to load in the lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vader = load_vader()\n",
    "vader.iloc[500:510, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Toronto Office of Recovery and Rebuild – Public Survey Results\n",
    "\n",
    "The more positive the polarity of a word, the more positive feeling the word evokes in the reader. All of the words in `vader` are all lowercase, while many of our tweets are not. We need to modify the text in the tweets so that the words in our tweets will match up with the words stored in `vader`. Additionally, we need to remove punctuation since that will cause the words to not match up as well. We will put these modified tweets into another column in our `DataFrame` so that we can still have access to them later.\n",
    "\n",
    "**Note about the dataset:** \n",
    "\n",
    "**Limitations**\n",
    "\n",
    "This dataset includes all survey records that were exported from the survey tool, including records that were started but not completed, i.e. partial completes. Respondent begins with #18 as #1-17 were test records and were removed. Also, this dataset has been cleaned to remove offensive language and personal data, along with those responses that may fall into the “offensive speech” category upon contextual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "params = { \"id\": \"toronto-office-of-recovery-and-rebuild-public-survey-results\"}\n",
    "package = requests.get(url, params = params).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[x for x in package[\"result\"][\"resources\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To get resource data:\n",
    "for idx, resource in enumerate(package[\"result\"][\"resources\"]):\n",
    "\n",
    "    # To get metadata for non datastore_active resources:\n",
    "    if resource[\"format\"] == \"CSV\":\n",
    "        url = base_url + \"/api/3/action/resource_show?id=\" + resource[\"id\"]\n",
    "        resource_data = requests.get(url).json()\n",
    "        # do a GET request on the url and access its text attribute\n",
    "        resource_dump_data = requests.get(resource_data['result']['url']).text\n",
    "        # read the raw csv text into a pandas dataframe to work with it\n",
    "        \n",
    "        public_survey_results = pd.read_csv(StringIO(resource_dump_data), sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "public_survey_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we only want the columns that have strings in their values - these will have dtype \"object\"\n",
    "text_cols = public_survey_results.columns[public_survey_results.dtypes.values == \"object\"]\n",
    "text_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "public_survey_results[text_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "There are a lot of NA values and some of these responses are words or phrases rather than sentences. What are some characteristcs of the columns that are more likely to contain free responses?\n",
    "\n",
    "**Hint:** Free text responses are more likely to be unique to the individual respondent and contain more characters than fields containing phrases or dropdown text responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the columns with the most unique responses\n",
    "public_survey_results[text_cols].apply(func = lambda x : x.nunique(), axis = 0).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# of the top ten, what are the median characters per response by column?\n",
    "top_text_cols = public_survey_results[text_cols].apply(func = lambda x : x.nunique(), axis = 0).sort_values(ascending = False)[:10].index.values\n",
    "average_length = {}\n",
    "for ttc in top_text_cols:\n",
    "    response_text = public_survey_results[ttc].dropna()\n",
    "    average_length[ttc] = np.median(response_text.apply(func = lambda x : len(x)))\n",
    "dict(sorted(average_length.items(), key=lambda item: item[1], reverse = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "It seems like the 'Tell us...' responses have the longest responses and are also among the top 10 unique response columns. Therefore, these will be an ideal set of columns for text analysis. But in their current state, they don't say much about what it is the survey administrators want to hear about. Let's try renaming them to something more descriptive using their position in the readme.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "public_survey_text = (public_survey_results[['Respondent', \n",
    "                                            '2. Tell us a bit about the choices you made above.(Please do not include any personal information (i.e. your name, telephone number, address) in your response.)',\n",
    "                                            '4. Tell us a bit about the choices you made above.\\n(Please do not include any personal information (i.e. your name, telephone number, address) in your response.)',\n",
    "                                            '6. Tell us how the choices you made above will help you, your community, your neighbourhood or the city.(Please do not include any personal information (i.e. your name, telephone number, address) in your response.)',\n",
    "                                            '8. Tell us more about this action you would like the City to consider in its recovery and rebuilding work?(Please do not include any personal information (i.e. your name, telephone number, address) in your response.)']]\n",
    "                      .rename(columns = {'Respondent' : 'id',\n",
    "                                         '2. Tell us a bit about the choices you made above.(Please do not include any personal information (i.e. your name, telephone number, address) in your response.)':'recover_rebuild_priority',\n",
    "                                         '4. Tell us a bit about the choices you made above.\\n(Please do not include any personal information (i.e. your name, telephone number, address) in your response.)':'city_fed_prov_govt_coord',\n",
    "                                         '6. Tell us how the choices you made above will help you, your community, your neighbourhood or the city.(Please do not include any personal information (i.e. your name, telephone number, address) in your response.)':'city_nongovt_coord',\n",
    "                                         '8. Tell us more about this action you would like the City to consider in its recovery and rebuilding work?(Please do not include any personal information (i.e. your name, telephone number, address) in your response.)':'civic_actions'\n",
    "                                        })\n",
    "                      .set_index('id'))\n",
    "public_survey_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NA with blank strings\n",
    "free_responses = public_survey_text.drop_duplicates().fillna(\"\")\n",
    "\n",
    "# Remove punctuation and lowercase all text\n",
    "free_responses_cleaned = free_responses.apply(lambda x : clean_string(x), axis = 0)\n",
    "\n",
    "free_responses_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Next, we want to merge our sentiment lexicon with our cleaned responses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**Question:** Use the readme to see which of the free text responses corresponded to each set of survey questions. What conclusions can you draw about polarity and responses? How does this compare with your assumptions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_polarities = []\n",
    "for col in free_responses_cleaned.columns.values:\n",
    "    polarity_df = compose_polarity(free_responses_cleaned.loc[:, col], vader)\n",
    "    polarity_df = polarity_df.rename(columns = {'polarity' : col})\n",
    "    response_polarities.append(polarity_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_polarities_df = pd.concat(response_polarities, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response_polarities_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
