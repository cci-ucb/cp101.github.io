{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Introduction to `geopandas`\n",
    "---\n",
    "\n",
    "Today we are going to learn how to use python and Jupyter notebooks to learn the basics of working with geospatial data in `geopandas`. `geopandas` is built on top of the `pandas` package you saw in the previous lab. Many of the methods you will see in this lab are shared between the two packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This line tells iPython to not display warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# RUN THIS CELL FIRST or the notebook won't work\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import requests\n",
    "import gtfs_kit as gk\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "import sys\n",
    "if sys.version_info[0] < 3: \n",
    "    from StringIO import StringIO\n",
    "else:\n",
    "    from io import StringIO\n",
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# These help the maps display nicely in the notebook\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [30, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "First, let's load our data and see what kind of data we are working with.  The `read_file` method requires that we pass the _filepath_ from our current directory (the location of this notebook) to the data. The `pwd()` function shows you the your current location in the file system. Another way you can say this is that it __p__rints your __w__orking __d__irectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "To get to the data, we would first enter the `lct_000b21a_e` folder from the working directory. The way we communicate this to the function is by passing `'lct_000b21a_e/lct_000b21a_e.shp'` as an argument. This folder contains geographic information organized by census tract for the entirety of Canada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "canada_shp = gpd.read_file('lct_000b21a_e/lct_000b21a_e.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Let's take a look at what our data looks like in Jupyter. We can use the `.head()` method to show the first 5 rows of our data. Similarly, you can use the `.tail()` method to show the last 5 rows of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "canada_shp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "This is nice, but we'd like to have some data to go with these polygons, and we'd like to just work within the Census Metropolitan Area of Toronto. Let's load some data from an earlier lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toronto_cma = pd.read_csv(\"~/git/cp101.github.io/labs/lab10/census21_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toronto_cma.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Important\n",
    "\n",
    "Many times over the course of reading and writing data from external sources, you might find that data types become corrupted. We can see that the GeoUID field in this dataframe ought to be right padded with zeroes so that it can match the expected form of the census tract ID in the geometry shapefile. Let's take care of that right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fix missingness and data types\n",
    "toronto_cma = toronto_cma.fillna(0)\n",
    "toronto_cma = toronto_cma.replace({'NA': 0})\n",
    "toronto_cma = toronto_cma.replace({'': 0})\n",
    "toronto_cma.iloc[:,4:] = toronto_cma.iloc[:,4:].apply(pd.to_numeric)\n",
    "# pad to get correct geouid length\n",
    "toronto_cma[\"GeoUID\"] = toronto_cma[\"GeoUID\"].astype(str).str.ljust(10, \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toronto_cma.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Much better. Now, let's do a left inner join to the GeoPandas dataframes so that the resulting object will retain its geometric properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toronto_gpd = canada_shp.merge(toronto_cma, how = \"inner\", left_on = \"CTUID\", right_on = \"GeoUID\")\n",
    "toronto_gpd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Examining the data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Accessing the data <font color='red'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The `.loc[]` and `.iloc[]` methods allow us to view cells in a `DataFrame` or `GeoDataFrame` based on their name or location. The __i__ in `.iloc[]` stands for the __integer__ position of a cell, and accesses cells by location in index coordinates. The `.loc[]` method allows you to access cells by the index of the rows and the names of the columns. For both `.loc[]` and `.iloc[]`, the first argument refers to the row, and the second argument refers to the column.\n",
    "\n",
    "Typically the row index will be the same as its integer position, but that is not always the case. Let's set up a `DataFrame` to see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = {'a': [1, 2, 3],\n",
    "                         'b': [4, 5, 6],\n",
    "                         'c': [7, 8, 9],\n",
    "                         'd': [10, 11, 12]},\n",
    "                 index = [1.1, 1.2, 1.3])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "If we want to access the number 8 from `df`, we would need to tell `.iloc[]` to look in `df` at row `1`, column `2` (remember that python starts counting from zero!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.iloc[1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Using `.loc[]`, we would need to tell the method that we want row index `1.2`, column name `'c'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[1.2, 'c']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "You can also specify a range of indices to both of these methods if you want to access multiple adjacent cells. For `iloc[]`, this range will always refer to integer coordinates of the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This code tells .iloc that we want rows 0 and 2, then all columns with index\n",
    "# greater than or equal to 1.\n",
    "df.iloc[[0, 2], 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We can also refer to a range named columns and rows using `.loc[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This code tells .loc that we want row 1.2, columns 'b' through 'd' inclusive.\n",
    "df.loc[[1.2], 'b':'d']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Basic maps <font color='red'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "The `geometry` column contains a new data type called a polygon, which is how `geopandas` is able to store geographic information. Let's look at one of these polygons using `.loc[]`.\n",
    "\n",
    "Is this a part of Toronto you recognize? Perhaps on a large park on the lakefront?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toronto_gpd.loc[7, 'geometry']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "Or perhaps this warped rectangle? A familiar university campus, near an urban park? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toronto_gpd.loc[55, 'geometry']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "To view all of the polygons in the `GeoDataFrame`, we can just use the `.plot()` method. The documentation for this function can be found [here](http://geopandas.org/mapping.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toronto_gpd.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "You can improve the appearance of this graph by removing the axis labels surrounding the map and adding a title. You may also see a line that says something like `<matplotlib.axes._subplots.AxesSubplot at [hexadecimal]>` above the map. You can prevent Jupyter from displaying this line by adding a `;` to the last line of code creating the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toronto_gpd.plot()\n",
    "\n",
    "# Adding the fontsize argument allows you to manipulate the font size.\n",
    "plt.title('Toronto CMA Census Tracts', fontsize = 30)\n",
    "\n",
    "# This turns the plot axes off.\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Creating a new shapefile\n",
    "    \n",
    "These are nice maps, but there are ways to make it more informative. For starters, the Toronto CMA is quite a large area with a high amount of variance in census tract area. This does not make for the most visually pleasing map, so for now let's continue on a previous theme and focus just on the City of Toronto. \n",
    "    \n",
    "We can do this by performing an inner join on the GeoUIDs of census tracts we queried in previous labs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "user_expressions": []
   },
   "outputs": [],
   "source": [
    "toronto_csd = pd.read_csv('~/git/cp101.github.io/labs/lab03/census21_data.csv')\n",
    "# fix missingness and data types\n",
    "toronto_csd = toronto_csd.fillna(0)\n",
    "toronto_csd = toronto_csd.replace({'NA': 0})\n",
    "toronto_csd = toronto_csd.replace({'': 0})\n",
    "# create toronto census tracts from this column\n",
    "toronto_census_tracts = toronto_csd[\"GeoUID\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a familiar number of census tracts!\n",
    "toronto_census_tracts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Let's try something a little different from our previous method of changing GeoUIDs to strings- a list comprehension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto_census_tracts_str = [str(x).ljust(10, \"0\") for x in toronto_census_tracts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "If we were going to write this list comprehension as a for loop, we would have to create a new list to save our converted data into. It would look a little something like this:\n",
    "```\n",
    "toronto_census_tracts_str = []\n",
    "for x in toronto_census_tracts:\n",
    "    toronto_census_tracts_str.append(str(x).ljust(10, \"0\"))\n",
    "```\n",
    "List comprehensions are so much more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Next, we are going to use `isin` to subset the rows of `toronto_gpd` to just the rows belonging to City of Toronto census tracts and create a new `GeoDataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new GeoDataFrame\n",
    "toronto = toronto_gpd[toronto_gpd['GeoUID'].isin(toronto_census_tracts_str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this should be True\n",
    "toronto.shape[0] == len(toronto_census_tracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Next, we can save our new `GeoDataFrame`. The following code saves `toronto` as a file type called a \"shapefile,\" which happens to be the same file type as the data we read in earlier. `geopandas` can also read and write many other geospatial file types, but we are just going to be using the same file type as before for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto.to_file(driver='ESRI Shapefile', filename = 'shapefiles/toronto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Now, we've been working with lengthy variable names for the last few labs. Let's use dictionaries to create some more user-friendly variables to type without having to sacrifice on well-formatted descriptions. One thing to make note of is that there is a typo in the CensusMapper API and that the vector `v_CA21_389: Average age` should really be `v_CA21_389: Median age`. APIs can be very useful, but always [check again](https://censusmapper.ca/api#api_variable) if something looks unusual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toronto.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "var_transforms = {'v_CA21_389: Average age':'medianage', \n",
    "                  'v_CA21_251: 65 years and over':'seniors',\n",
    "                  'v_CA21_8: Total - Age':'tot_age',\n",
    "                  'v_CA21_1040: Prevalence of low income based on the Low-income measure, after tax (LIM-AT) (%)':'lim_at',\n",
    "                  'v_CA21_1085: Prevalence of low income based on the Low-income cut-offs, after tax (LICO-AT) (%)':'lico_at',\n",
    "                  'v_CA21_1140: Gini index on adjusted household total income':'gini_total',\n",
    "                  'v_CA21_1141: Gini index on adjusted household market income':'gini_market',\n",
    "                  'v_CA21_1142: Gini index on adjusted household after-tax income':'gini_after_tax'\n",
    "                 }\n",
    "\n",
    "toronto = toronto.rename(columns = var_transforms)\n",
    "\n",
    "toronto['pct_seniors'] = toronto['seniors'] / toronto['tot_age']\n",
    "\n",
    "plot_titles = {'medianage':'Median age by census tract', \n",
    "               'pct_seniors':'Percent of population 65 years and over by census tract',\n",
    "               'lim_at':'Prevalence of low income based on the LIM-AT (%) by census tract',\n",
    "               'lico_at':'Prevalence of low income based on the LICO-AT (%) by census tract',\n",
    "               'gini_total':'Gini index on adjusted household total income by census tract',\n",
    "               'gini_market':'Gini index on adjusted household market income by census tract',\n",
    "               'gini_after_tax':'Gini index on adjusted household after-tax income by census tract'\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Now we can make a map with our new data. In fact, we can make a lot of maps with our new data. So let's make a function to avoid rewriting the same code over and over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_plot(plot_var):\n",
    "    toronto.plot(column = plot_var, legend = True)\n",
    "    plt.title(plot_titles[plot_var], fontsize = 30)\n",
    "    plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_plot('medianage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Can you tell which census tracts are likely to be near the University of Toronto campus, just from looking at this map? If yes, why might that be? If no, what other information would you need in order to do so?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "If we want to make this map more informative, we can incorporate some of the other columns from the data into the map. For example, the `v_CA21_1142` vector contains [Gini index](https://www.physics.ucla.edu/~chester/GINI/index.html) of each census tract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "We can pass the column name as an optional argument to the `.plot()` method to create a map with a color gradient based on the values in this column. Setting `legend` equal to `True` in the function call will also tell the method that we want our final plot to have a legend so that we can interpret the colors on the map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "simple_plot('gini_after_tax')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "If you want to change the color scheme of the map, you can choose the \"color map\" of the plot by adding in the optional `cmap` argument to `plot()`. You can choose from the variety of color maps available in the `matplotlib` package, which is what our plotting software is built off of. Above, we saw the default colormap, which is called viridis. If you want to view all the available colormaps, you can reference the documentation [here](https://matplotlib.org/users/colormaps.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toronto.plot(column = 'gini_after_tax', legend = True, cmap = 'magma')\n",
    "plt.title(plot_titles['gini_after_tax'], fontsize = 30)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Now it's your turn! In the following cell, try creating your own choropleth map using a different column from `toronto_gpd`. If you want to see all of the columns in our data, you can scroll up to where we displayed all of the column names.\n",
    "\n",
    "<font color = 'red'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "toronto.plot(column = 'lico_at', legend = True, cmap = 'Wistia')\n",
    "plt.title(plot_titles['lico_at'], fontsize = 30)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some conclusions that you can draw from the map you created?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "## Coordinate Reference Systems (CRS)\n",
    "\n",
    "Statistics Canada uses a special Coordinate Reference System called [Statistics Canada Lambert](https://epsg.io/3347) (EPSG:3347) for its shapefiles. You can read Statistics Canada's Illustrated Glossary page, including why this projection is used for most statistical analysis of Canadian geographies [here](https://www150.statcan.gc.ca/n1/pub/92-195-x/2021001/other-autre/coord/coord-eng.htm). \n",
    "\n",
    "\n",
    "A standard CRS used internationally for a variety of catography and satellite navigateion is the World Geodetic System. The Map and Data Library has several Toronto-centric datasets- pay special attention to their projection before working with them. The EPSG values are shortened abbreviations of the coordinate system names that can be used to easily convert between projections using geospatial software/libraries. You can read more about EPSG [here](https://epsg.org/home.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toronto.crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# reprojects coordinates to new coordinate reference system\n",
    "toronto = toronto.to_crs(4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toronto.plot()\n",
    "plt.title('City of Toronto Census Tracts', fontsize = 30)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# More geospatial data types in `geopandas`\n",
    "\n",
    "So far, we have only been creating maps using polygons, but `geopandas` has a few more data types we can work with. First, we need some new data to work with. In the `ttc` folder, we have folders named `bart_stations` and `bart_routes` containing geospatial data about the BART system. Load in the data as we did with `alameda` file from the beginning of the lab.\n",
    "\n",
    "### Working with GTFS data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" Utility functions for GGR337 CKAN API lab.\n",
    "\n",
    "These functions have been adapted from Data 100 course materials.\n",
    "\"\"\"\n",
    "\n",
    "def retrieve_package(keys):\n",
    "    \"\"\"Downloads package info of a dataset using CKAN's API\n",
    "\n",
    "    Args:\n",
    "        keys (dict): A Python dictionary with Toronto Open Dataset id \n",
    "          keys (strings), like this (but filled in):\n",
    "            {\n",
    "                \"id\": \"<dataset id here>\",\n",
    "            }\n",
    "\n",
    "    Returns:\n",
    "        list: A list of Dictonary objects, each representing help, response, and result.\"\"\"\n",
    "    import requests\n",
    "    # Toronto Open Data is stored in a CKAN instance. It's APIs are documented here:\n",
    "    # https://docs.ckan.org/en/latest/api/\n",
    "    base_url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca\"\n",
    "    # Datasets are called \"packages\". Each package can contain many \"resources\"\n",
    "    # To retrieve the metadata for this package and its resources, use the package name in this page's URL:\n",
    "    url = base_url + \"/api/3/action/package_show\"\n",
    "    package = requests.get(url, params = keys).json()\n",
    "    return package\n",
    "\n",
    "\n",
    "def load_tpl_events(method):\n",
    "    \"\"\"Loads the dataframe of the toronto public library events feeding using the predownloaded csv or API request\n",
    "    \n",
    "    Args:\n",
    "        method (string): can either be 'read_csv' for predownloaded csv or 'API' for API request\n",
    "        \"\"\"\n",
    "        \n",
    "    if method == \"read_csv\":\n",
    "        return pd.read_csv(\"tpl-events-feed.csv\")\n",
    "    elif method == \"API\":\n",
    "        \n",
    "        package = retrieve_package(\"library-branch-programs-and-events-feed\")\n",
    "        \n",
    "        \n",
    "        for idx, resource in enumerate(package[\"result\"][\"resources\"]):\n",
    "            if resource[\"datastore_active\"]:\n",
    "                # to get all records in CSV format\n",
    "                url = base_url + \"/datastore/dump/\" + resource[\"id\"]\n",
    "                # do a GET request on the url and access its text attribute\n",
    "                resource_dump_data = requests.get(url).text\n",
    "                # read the raw csv text into a pandas dataframe to work with it\n",
    "                return pd.read_csv(StringIO(resource_dump_data), sep=\",\")\n",
    "    else:\n",
    "        print(\"Unacceptable argument for 'method'. Use either 'read_csv' or 'API'.\")\n",
    "        return\n",
    "              \n",
    "def load_public_survey(method):\n",
    "    \"\"\"Loads the dataframe of the toronto public survey results using the predownloaded csv or API request\n",
    "    \n",
    "    Args:\n",
    "        method (string): can either be 'read_csv' for predownloaded csv or 'API' for API request\n",
    "        \"\"\"\n",
    "        \n",
    "    if method == \"read_csv\":\n",
    "        return pd.read_csv(\"torr-public-survey-results.csv\")\n",
    "    elif method == \"API\":\n",
    "        \n",
    "        package = retrieve_package({ \"id\": \"toronto-office-of-recovery-and-rebuild-public-survey-results\"})\n",
    "        \n",
    "        for idx, resource in enumerate(package[\"result\"][\"resources\"]):\n",
    "           # To get metadata for non datastore_active resources:\n",
    "            if resource[\"format\"] == \"CSV\":\n",
    "                url = base_url + \"/api/3/action/resource_show?id=\" + resource[\"id\"]\n",
    "                resource_data = requests.get(url).json()\n",
    "                # do a GET request on the url and access its text attribute\n",
    "                resource_dump_data = requests.get(resource_data['result']['url']).text\n",
    "                # read the raw csv text into a pandas dataframe to work with it\n",
    "                return pd.read_csv(StringIO(resource_dump_data), sep=\",\")\n",
    "    else:\n",
    "        print(\"Unacceptable argument for 'method'. Use either 'read_csv' or 'API'.\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://open.toronto.ca/dataset/ttc-routes-and-schedules/\n",
    "# https://open.toronto.ca/dataset/major-city-wide-cycling-routes/\n",
    "\n",
    "ttc_routes_package = retrieve_package({\"id\":\"ttc-routes-and-schedules\"})\n",
    "base_url = \"https://ckan0.cf.opendata.inter.prod-toronto.ca\"\n",
    "for idx, resource in enumerate(ttc_routes_package[\"result\"][\"resources\"]):\n",
    "    # To get metadata for non datastore_active resources:\n",
    "    if not resource[\"datastore_active\"]:\n",
    "        url = base_url + \"/api/3/action/resource_show?id=\" + resource[\"id\"]\n",
    "        resource_data = requests.get(url).json()\n",
    "zipurl = resource_data['result']['url']\n",
    "zipresp = urlopen(zipurl)\n",
    "# Create a new file on the hard drive\n",
    "tempzip = open(\"ttc/tempfile.zip\", \"wb\")\n",
    "tempzip.write(zipresp.read())\n",
    "# Close the newly-created file\n",
    "tempzip.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a lot is going on here, but it is just reading in the contents of the zipped file to dataframes\n",
    "# adapted from: https://max-coding.medium.com/how-to-process-gtfs-data-using-pandas-geopandas-4b34f2ad3273\n",
    "\n",
    "with ZipFile(\"ttc/routes_schedules.zip\") as myzip:\n",
    "    shapes_df = pd.read_csv(myzip.open(\"shapes.txt\"), dtype={\n",
    "        'shape_id': 'str', \n",
    "        'shape_pt_lat': 'float', \n",
    "        'shape_pt_lon': 'float',  \n",
    "        'shape_pt_sequence': 'Int64', \n",
    "        'shape_dist_traveled': 'float'\n",
    "    })\n",
    "    shapes_gdf = gpd.GeoDataFrame(shapes_df, \n",
    "        geometry=gpd.points_from_xy(shapes_df.shape_pt_lon, shapes_df.shape_pt_lat)).set_crs(epsg=4326)\n",
    "    \n",
    "    stops_df = pd.read_csv(myzip.open(\"stops.txt\"), dtype={\n",
    "        'stop_id': 'str', \n",
    "        'stop_code': 'str',\n",
    "        'stop_name': 'str',\n",
    "        'stop_lat': 'float',\n",
    "        'stop_lon': 'float'\n",
    "    })\n",
    "    stops_gdf = gpd.GeoDataFrame(stops_df, \n",
    "        geometry=gpd.points_from_xy(stops_df.stop_lon, stops_df.stop_lat)).set_crs(epsg=4326)\n",
    "    \n",
    "    routes_df = pd.read_csv(myzip.open(\"routes.txt\"), dtype={\n",
    "        'route_id': 'str',  \n",
    "        'agency_id': 'str',  \n",
    "        'route_short_name': 'str',  \n",
    "        'route_long_name': 'str', \n",
    "        'route_desc': 'str', \n",
    "        'route_type': 'Int64',\n",
    "        'route_color': 'str',  \n",
    "        'route_text_color': 'str'\n",
    "    })\n",
    "    \n",
    "    trips_df = pd.read_csv(myzip.open(\"trips.txt\"), dtype={\n",
    "        'route_id': 'str', \n",
    "        'service_id': 'str',  \n",
    "        'trip_id': 'str',\n",
    "        'shape_id': 'str', \n",
    "        'trip_headsign': 'str', \n",
    "        'direction_id': 'str',  \n",
    "        'block_id': 'str', \n",
    "        'wheelchair_accessible': 'str', \n",
    "        'bikes_allowed': 'str'\n",
    "    })\n",
    "    \n",
    "    calendar_df = pd.read_csv(myzip.open(\"calendar.txt\"), dtype={\n",
    "        'service_id': 'str',  \n",
    "        'monday': 'bool',  \n",
    "        'tuesday': 'bool',  \n",
    "        'wednesday': 'bool',  \n",
    "        'thursday': 'bool',  \n",
    "        'friday': 'bool', \n",
    "        'saturday': 'bool',  \n",
    "        'sunday': 'bool',  \n",
    "        'start_date': 'str', \n",
    "        'end_date': 'str',\n",
    "    })\n",
    "    \n",
    "    calendar_dates_df = pd.read_csv(myzip.open(\"calendar_dates.txt\"), dtype={\n",
    "        'service_id': 'str',  \n",
    "        'date': 'str',\n",
    "        'exception_type': 'Int64',\n",
    "    })\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shapes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "segments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stops_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "routes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trips_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "From this table, we can see that there are 9 lines in the Toronto TTC file and quite a few of them haven't run over the period for which we have data. `service_id == 1` runs weekdays, `service_id == 2` runs Saturdays, and `service_id == 3` runs Sundays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar_dates_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "These dataframes are quite large and plotting maps of them can get quite messy, so let's pick one day of the week to focus on. You can change this to select any day of the week that you want, just be sure to type it in `lowercase`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defaults to whatever day of the week it is when you run this cell, but you can manually change it to whatever you like!\n",
    "day_of_week_name = dt.datetime.today().strftime('%A').lower()\n",
    "day_of_week_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# TODO: explain the motivation for the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "coords = shapes_df[[\"shape_pt_lat\", \"shape_pt_lon\", \"shape_pt_sequence\"]]\n",
    "coords_roll_1 = np.roll(coords, 1, axis=0)\n",
    "segments = pd.DataFrame(np.concatenate([coords_roll_1, coords], axis=1), columns=[\"start_lat\", \"start_lng\", \"start_seq\", \"end_lat\", \"end_lng\", \"end_seq\"])\n",
    "segments_df = shapes_df[[\"shape_id\"]].join(segments)\n",
    "segments_df = segments_df[segments_df.end_seq != 1]\n",
    "segments_df = segments_df.drop(columns=['end_seq']).rename(columns={ \"start_seq\": \"seq\" })\n",
    "segments_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# TODO: explain motivation for the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shape_trips = trips_df.groupby(by=\"shape_id\").size().to_frame(\"tot_trips\")\n",
    "shape_trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shape_trips[shape_trips.index == \"982620\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "# TODO: explain motivation for the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def limit_to_bounding_box(gdf, bounding_box):\n",
    "    return gdf.cx[bounding_box[\"west\"]:bounding_box[\"east\"],bounding_box[\"south\"]:bounding_box[\"north\"]]\n",
    "\n",
    "\n",
    "toronto_bb = {\n",
    "    \"north\": min(toronto.bounds['maxy']),\n",
    "    \"south\": min(toronto.bounds['miny']),\n",
    "    \"west\": min(toronto.bounds['maxx']),\n",
    "    \"east\": min(toronto.bounds['minx']),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# want to join trip counts by shape_id to the segments\n",
    "shape_segments_trips = pd.merge(segments_df, shape_trips, left_on=\"shape_id\", right_index=True)\n",
    "segment_trips = shape_segments_trips.groupby(by=[\"start_lat\", \"start_lng\", \"end_lat\", \"end_lng\"]).sum(\"tot_trips\").reset_index()\n",
    "segment_trips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# TODO: explain motivation for next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from shapely.geometry import Point\n",
    "from shapely.geometry import LineString\n",
    "\n",
    "segment_trips['start_point'] = segment_trips.apply(lambda row: Point(row['start_lng'], row['start_lat']), axis = 1)\n",
    "segment_trips['end_point'] = segment_trips.apply(lambda row: Point(row['end_lng'], row['end_lat']), axis = 1)\n",
    "\n",
    "segment_trips['geometry'] = segment_trips.apply(lambda row: LineString([row['start_point'], row['end_point']]), axis = 1)\n",
    "\n",
    "segment_trips.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "segment_trips_gpd = limit_to_bounding_box(gpd.GeoDataFrame(segment_trips), toronto_bb)\n",
    "segment_trips_gpd.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "First, we need the base map. We can use the aggregated `GeoDataFrame` we created earlier in this notebook. Try setting the `color` and `edgecolor` arguments so the map looks more uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = toronto.plot()\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Next, overlay the routes and stations and plot the graph by using this as the base for the map. The Jupyter notebook will not remember the map you drew in the previous cell even if you assigned it a name, so make sure to plot the base map again in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "segment_trips_gpd.plot(ax=base, color = '#789b73', linewidth = 6, zorder=2)\n",
    "stops_gdf.plot(ax=base, color = '#cdfd02', markersize = 25, zorder=3)\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,21))\n",
    "base = toronto.plot(color = '#d8dcd6', zorder=1)\n",
    "segment_trips_gpd.plot(cmap=\"plasma\", column='tot_trips', legend=True)\n",
    "plt.title(f\"Number of trips on TTC\", fontsize=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You're done with this lab! If you are interested in learning more about what you can do with `geopandas`, you can find the documentation for the package [here](http://geopandas.org/reference.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "### Adapted from Berkeley lab content created by Monica Wilkinson and Vera Wang\n",
    "### References:\n",
    "- http://geopandas.org/mapping.html\n",
    "- https://matplotlib.org/users/colormaps.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
